{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a80245",
   "metadata": {},
   "source": [
    "# Modelling and Deployment using MLOps \n",
    "\n",
    "Now that we have audio input data & corresponding labels in an array format, it is easier to consume and apply Natural language processing techniques. We can convert audio files labels into integers using label Encoding or One Hot Vector Encoding for machines to learn. The labeled dataset will help us in the neural network model output layer for predicting results. These help in training & validation datasets into nD array.\n",
    "At this stage, we apply other pre-processing techniques like dropping columns, normalization, etc. to conclude our final training data for building models. Moving to the next stage of splitting the dataset into train, test, and validation is what we have been doing for other models. \n",
    "We can leverage CNN, RNN, LSTM,CTC etc. deep neural algorithms to build and train the models for speech applications like speech recognition. The model trained with the standard size few seconds audio chunk transformed into an array of n dimensions with the respective labels will result in predicting output labels for test audio input. As output labels will vary beyond binary, we are talking about building a multi-class label classification method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aaa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "import tensorflow as tf\n",
    "from clean import Clean\n",
    "from utils import vocab\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler\n",
    "from evaluator import CallbackEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39c85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78298269",
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_ALPHABET='ሀለሐመሠረሰቀበግዕዝተኀነአከወዐዘየደገጠጰጸፀፈፐቈኈጐኰፙፘፚauiāeəo'\n",
    "EN_ALPHABET='abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d2ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 11:33:33,938:logger:Successfully initialized clean class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n"
     ]
    }
   ],
   "source": [
    "cleaner = Clean()\n",
    "char_to_num,num_to_char=vocab(EN_ALPHABET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81812",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880d08c",
   "metadata": {},
   "source": [
    "**objective**: Build a Deep learning model that converts speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f636c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_df = pd.read_csv(\"../data/swahili.csv\")\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe74d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f37929",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_preprocessed = pre_model.preprocessing_learn(swahili_df,'key','file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83b3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_preprocessed = pre_model.preprocessing_learn(amharic_df,'key','file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00fc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df = swahili_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6ecf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(train_df[\"file\"]), list(train_df[\"text\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(val_df[\"file\"]), list(val_df[\"text\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baf182",
   "metadata": {},
   "source": [
    "## LSTM Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5ee6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 2)]                           0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 2, 1)                          0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " reshape (Reshape)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 2)                             30               \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 2)                             6                \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dropout (Dropout)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dense (Dense)                                   (None, None, 45)                            135              \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 195\n",
      "Trainable params: 187\n",
      "Non-trainable params: 8\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['mfcc-0'])\n",
    "fft_length = 2\n",
    "model = learn.build_asr_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=1,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87fcda",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b50a3",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b59de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/4 [======>.......................] - ETA: 6:26 - loss: 82958.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 11:36:10.355024: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15851520 exceeds 10% of free system memory.\n",
      "2022-06-02 11:36:10.355161: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15851520 exceeds 10% of free system memory.\n",
      "2022-06-02 11:36:10.359330: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15851520 exceeds 10% of free system memory.\n",
      "2022-06-02 11:36:19.113986: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 13086720 exceeds 10% of free system memory.\n",
      "2022-06-02 11:36:19.133298: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 13086720 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - ETA: 0s - loss: 101282.0547  The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "The vocabulary is: ['', 'ሀ', 'ለ', 'ሐ', 'መ', 'ሠ', 'ረ', 'ሰ', 'ቀ', 'በ', 'ግ', 'ዕ', 'ዝ', 'ተ', 'ኀ', 'ነ', 'አ', 'ከ', 'ወ', 'ዐ', 'ዘ', 'የ', 'ደ', 'ገ', 'ጠ', 'ጰ', 'ጸ', 'ፀ', 'ፈ', 'ፐ', 'ቈ', 'ኈ', 'ጐ', 'ኰ', 'ፙ', 'ፘ', 'ፚ', 'a', 'u', 'i', 'ā', 'e', 'ə', 'o'] (size =44)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Word Error Rate: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target    : በለጠየበየነየበነ\n",
      "Prediction: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target    : በበሀአአነበረ\n",
      "Prediction: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "4/4 [==============================] - 687s 186s/step - loss: 101282.0547 - val_loss: 98912.0469\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(model,validation_dataset)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6d289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
