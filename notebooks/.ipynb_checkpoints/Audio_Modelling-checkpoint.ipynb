{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a80245",
   "metadata": {},
   "source": [
    "# Modelling and Deployment using MLOps \n",
    "\n",
    "Now that we have audio input data & corresponding labels in an array format, it is easier to consume and apply Natural language processing techniques. We can convert audio files labels into integers using label Encoding or One Hot Vector Encoding for machines to learn. The labeled dataset will help us in the neural network model output layer for predicting results. These help in training & validation datasets into nD array.\n",
    "At this stage, we apply other pre-processing techniques like dropping columns, normalization, etc. to conclude our final training data for building models. Moving to the next stage of splitting the dataset into train, test, and validation is what we have been doing for other models. \n",
    "We can leverage CNN, RNN, LSTM,CTC etc. deep neural algorithms to build and train the models for speech applications like speech recognition. The model trained with the standard size few seconds audio chunk transformed into an array of n dimensions with the respective labels will result in predicting output labels for test audio input. As output labels will vary beyond binary, we are talking about building a multi-class label classification method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75aaa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "import tensorflow as tf\n",
    "from clean import Clean\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78298269",
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_ALPHABET='ሀለሐመሠረሰቀበግዕዝተኀነአከወዐዘየደገጠጰጸፀፈፐቈኈጐኰፙፘፚauiāeəo'\n",
    "EN_ALPHABET='abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9567228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 04:05:54,680:logger:Successfully initialized clean class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] (size =27)\n"
     ]
    }
   ],
   "source": [
    "cleaner = Clean()\n",
    "vocabs = cleaner.vocab(EN_ALPHABET)\n",
    "char_to_num,num_to_char = vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81812",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880d08c",
   "metadata": {},
   "source": [
    "**objective**: Build a Deep learning model that converts speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f636c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_df = pd.read_csv(\"../data/swahili.csv\")\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")\n",
    "# swahili_df['key'] = list(range(len(swahili_df)))\n",
    "# amharic_df['key'] = list(range(len(amharic_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a02b1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 384\n",
    "\n",
    "\n",
    "def encode_single_sample(wav_file, label,type='amharic'):\n",
    "    ###########################################\n",
    "    ##  Process the Audio\n",
    "    ##########################################\n",
    "    # 1. Read wav file\n",
    "    file=None\n",
    "    if type == 'swahili':\n",
    "        wavs_path='../data/swahili_train_wav/'\n",
    "        wav_file_ = wav_file+\".wav\"\n",
    "        for inner_wav_folder in os.listdir(wavs_path):\n",
    "            for fetched_file in inner_wav_folder:\n",
    "                if fetched_file == wav_file_:\n",
    "                    file = tf.io.read_file(wavs_path + wav_file_)\n",
    "    else:\n",
    "        wavs_path='../data/amharic_train_wav//'\n",
    "        wav_file_ = wav_file+\".wav\"\n",
    "        file = tf.io.read_file(wavs_path + wav_file_)\n",
    "\n",
    "    # 2. Decode the wav file\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    # 3. Change type to float\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    # 4. Get the spectrogram\n",
    "    spectrogram = tf.signal.stft(\n",
    "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
    "    )\n",
    "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "    # 6. normalisation\n",
    "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "    ###########################################\n",
    "    ##  Process the label\n",
    "    ##########################################\n",
    "    # 7. Convert label to Lower case\n",
    "    label = tf.strings.lower(label)\n",
    "    # 8. Split the label\n",
    "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
    "    # 9. Map the characters in label to numbers\n",
    "    label = char_to_num(label)\n",
    "    # 10. Return a dict as our model is expecting two inputs\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26c30d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_17023/2553443906.py\", line 20, in encode_single_sample  *\n        if fetched_file == wav_file_:\n\n    ValueError: 'file' is None at the end of the else branch.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define the trainig dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[1;32m      4\u001b[0m     (\u001b[38;5;28mlist\u001b[39m(swahili_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mlist\u001b[39m(swahili_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode_single_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mpadded_batch(batch_size)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define the validation dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;28mlist\u001b[39m(swahili_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mlist\u001b[39m(swahili_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2050\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2048\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5284\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m   5283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m-> 5284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5288\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5290\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2559\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[1;32m   2561\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2567\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2569\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2533\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   2535\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2536\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m   cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mgeneralize(cache_key)\n\u001b[1;32m   2709\u001b[0m   (args, kwargs) \u001b[38;5;241m=\u001b[39m cache_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                          graph_function)\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2624\u001b[0m ]\n\u001b[1;32m   2625\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   2626\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   2638\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   2639\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1141\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1141\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m   1146\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:248\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    243\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[1;32m    245\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:177\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    176\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 177\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n\u001b[1;32m    179\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ret)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filew05olet1.py:73\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encode_single_sample\u001b[0;34m(wav_file, label, type)\u001b[0m\n\u001b[1;32m     71\u001b[0m fetched_file \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfetched_file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m wavs_path \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwavs_path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mswahili\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m (audio, _) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mdecode_wav, (ag__\u001b[38;5;241m.\u001b[39mld(file),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     75\u001b[0m audio \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39msqueeze, (ag__\u001b[38;5;241m.\u001b[39mld(audio),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1341\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1339\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1341\u001b[0m   \u001b[43m_py_if_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morelse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1394\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1393\u001b[0m   \u001b[38;5;124;03m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cond \u001b[38;5;28;01melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filew05olet1.py:64\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encode_single_sample.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m fetched_file \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfetched_file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m inner_wav_folder \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner_wav_folder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavs_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner_wav_folder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:449\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    445\u001b[0m   _tf_distributed_iterable_for_stmt(\n\u001b[1;32m    446\u001b[0m       iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m   \u001b[43m_py_for_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:498\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m iter_:\n\u001b[0;32m--> 498\u001b[0m     \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:464\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 464\u001b[0m   \u001b[43moriginal_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotected_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m   after_iteration()\n\u001b[1;32m    466\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filew05olet1.py:61\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encode_single_sample.<locals>.if_body_1.<locals>.loop_body_1\u001b[0;34m(itr_1)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(fetched_file) \u001b[38;5;241m==\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(wav_file_), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_wav_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfetched_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:449\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    445\u001b[0m   _tf_distributed_iterable_for_stmt(\n\u001b[1;32m    446\u001b[0m       iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m   \u001b[43m_py_for_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:498\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m iter_:\n\u001b[0;32m--> 498\u001b[0m     \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:464\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 464\u001b[0m   \u001b[43moriginal_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotected_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m   after_iteration()\n\u001b[1;32m    466\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filew05olet1.py:60\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__encode_single_sample.<locals>.if_body_1.<locals>.loop_body_1.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m file\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetched_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_file_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1339\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;66;03m# Note: tf.cond doesn't support SparseTensor.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mis_dense_tensor(cond):\n\u001b[0;32m-> 1339\u001b[0m   \u001b[43m_tf_if_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morelse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:1385\u001b[0m, in \u001b[0;36m_tf_if_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     _verify_tf_cond_vars(new_body_vars_[\u001b[38;5;241m0\u001b[39m], new_orelse_vars, symbol_names)\n\u001b[1;32m   1383\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m new_orelse_vars\n\u001b[0;32m-> 1385\u001b[0m final_cond_vars \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_orelse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m final_cond_vars \u001b[38;5;241m=\u001b[39m final_cond_vars \u001b[38;5;241m+\u001b[39m init_vars[nouts:]\n\u001b[1;32m   1389\u001b[0m set_state(final_cond_vars)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/tensorflow/python/autograph/operators/control_flow.py:323\u001b[0m, in \u001b[0;36mverify_single_cond_var\u001b[0;34m(name, body_var, orelse_var)\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is None at the end of the main branch.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orelse_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is None at the end of the else branch.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body_var, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m    327\u001b[0m   body_var \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor_v2(body_var)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_17023/2553443906.py\", line 20, in encode_single_sample  *\n        if fetched_file == wav_file_:\n\n    ValueError: 'file' is None at the end of the else branch.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(swahili_df[\"key\"]), list(swahili_df[\"text\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(swahili_df[\"key\"]), list(swahili_df[\"text\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7188df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c907296",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_preprocessed = pre_model.preprocessing_learn(swahili_df,'text','mfcc-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92e2f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_preprocessed = pre_model.preprocessing_learn(amharic_df,'text','mfcc-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06d91ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df = swahili_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7f886a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>duration</th>\n",
       "      <th>rate</th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc-1</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc-11</th>\n",
       "      <th>mfcc-12</th>\n",
       "      <th>mfcc-13</th>\n",
       "      <th>mfcc-14</th>\n",
       "      <th>mfcc-15</th>\n",
       "      <th>mfcc-16</th>\n",
       "      <th>mfcc-17</th>\n",
       "      <th>mfcc-18</th>\n",
       "      <th>mfcc-19</th>\n",
       "      <th>mfcc-0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.140494</td>\n",
       "      <td>0.315876</td>\n",
       "      <td>1866.409879</td>\n",
       "      <td>1589.100426</td>\n",
       "      <td>3359.724832</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>[83.27722, 99.38664, 102.47901, 90.34943, 71....</td>\n",
       "      <td>...</td>\n",
       "      <td>[8.168702, 14.0366535, 16.038925, 15.536394, ...</td>\n",
       "      <td>[-3.862731, -4.396248, 5.0084095, 10.777882, ...</td>\n",
       "      <td>[-5.1126146, -8.593059, 0.5960377, 3.3219213,...</td>\n",
       "      <td>[7.466035, 8.751228, 11.057912, 8.758237, 9.4...</td>\n",
       "      <td>[7.371366, 6.87022, 10.244551, 11.825554, 6.3...</td>\n",
       "      <td>[-3.9841778, -6.7652984, 0.2561152, 8.012002,...</td>\n",
       "      <td>[-10.074699, -8.62624, -2.4564838, 3.956565, ...</td>\n",
       "      <td>[-9.044133, -6.4383917, -0.39757836, 2.210609...</td>\n",
       "      <td>[-5.1926327, -0.0011684299, 6.8749275, 8.6460...</td>\n",
       "      <td>[-499.7574, -478.80606, -484.01407, -501.7097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.162743</td>\n",
       "      <td>0.321692</td>\n",
       "      <td>1900.991630</td>\n",
       "      <td>1655.337451</td>\n",
       "      <td>3481.883645</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>[41.19271, 43.64393, 39.571014, 39.280922, 37...</td>\n",
       "      <td>...</td>\n",
       "      <td>[12.310079, 11.866571, 14.6186, 13.844271, 15...</td>\n",
       "      <td>[9.388332, 9.306625, 11.078758, 10.565126, 13...</td>\n",
       "      <td>[6.2646713, 6.783835, 7.9363847, 7.3178806, 1...</td>\n",
       "      <td>[3.6419904, 4.6776457, 6.5209513, 5.654462, 9...</td>\n",
       "      <td>[2.104116, 3.356882, 5.659413, 4.1505423, 7.4...</td>\n",
       "      <td>[1.8616735, 3.0369122, 4.5197396, 2.1954632, ...</td>\n",
       "      <td>[2.6579711, 3.5943644, 4.0682106, 1.5907955, ...</td>\n",
       "      <td>[3.8751554, 4.5339994, 4.1353364, 2.3035893, ...</td>\n",
       "      <td>[4.78513, 5.23909, 3.1639218, 2.1374843, 3.07...</td>\n",
       "      <td>[-513.5918, -511.832, -514.46265, -514.7265, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.164615</td>\n",
       "      <td>0.355232</td>\n",
       "      <td>2063.848819</td>\n",
       "      <td>1775.474533</td>\n",
       "      <td>3859.115658</td>\n",
       "      <td>0.049307</td>\n",
       "      <td>[82.287254, 87.646355, 82.0253, 94.94374, 91....</td>\n",
       "      <td>...</td>\n",
       "      <td>[5.2754736, 3.1030126, 13.407595, 11.497463, ...</td>\n",
       "      <td>[-3.8446255, -2.971214, -4.5158086, -15.05637...</td>\n",
       "      <td>[-10.348606, -7.0784025, -4.1666775, -4.44252...</td>\n",
       "      <td>[-6.6246963, -4.2140656, 7.8236904, 18.939777...</td>\n",
       "      <td>[3.9710789, 5.172476, 7.676926, 8.67643, 14.1...</td>\n",
       "      <td>[8.910217, 11.790334, 7.967798, 1.5910062, 2....</td>\n",
       "      <td>[1.8036795, 5.0296683, 7.0958915, 3.4036198, ...</td>\n",
       "      <td>[-5.9104943, -4.2941203, -1.3336377, -5.77136...</td>\n",
       "      <td>[-4.1684318, -3.8071477, -3.5682287, -4.74488...</td>\n",
       "      <td>[-474.19022, -459.1682, -450.55014, -418.1049...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.451515</td>\n",
       "      <td>1852.997999</td>\n",
       "      <td>1686.271162</td>\n",
       "      <td>3497.119321</td>\n",
       "      <td>0.048648</td>\n",
       "      <td>[119.83493, 107.27291, 75.60173, 67.41678, 67...</td>\n",
       "      <td>...</td>\n",
       "      <td>[6.872613, 13.56502, 21.499748, 20.410894, 19...</td>\n",
       "      <td>[1.4193982, 6.754388, 12.58844, 9.958288, 8.7...</td>\n",
       "      <td>[8.313048, 11.108404, 11.795284, 11.0210495, ...</td>\n",
       "      <td>[6.1494207, 13.033355, 11.653352, 12.482355, ...</td>\n",
       "      <td>[-0.7938506, 2.4656396, 4.9645514, 4.650465, ...</td>\n",
       "      <td>[2.9275475, 3.0828507, 6.777918, 4.701215, 4....</td>\n",
       "      <td>[1.3701015, 6.540661, 12.440676, 9.548298, 9....</td>\n",
       "      <td>[-1.563393, 1.938827, 7.003166, 6.8333797, 7....</td>\n",
       "      <td>[4.3837843, 3.7773314, 3.6287487, 6.9111385, ...</td>\n",
       "      <td>[-378.3314, -411.32068, -453.55417, -462.8506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.154860</td>\n",
       "      <td>0.319131</td>\n",
       "      <td>2039.119501</td>\n",
       "      <td>1727.509904</td>\n",
       "      <td>3764.721680</td>\n",
       "      <td>0.053856</td>\n",
       "      <td>[89.52852, 70.175644, 45.881165, 45.599052, 4...</td>\n",
       "      <td>...</td>\n",
       "      <td>[4.9950647, 8.634994, 14.272382, 12.56731, 12...</td>\n",
       "      <td>[7.4182777, 7.690419, 12.692128, 12.182736, 1...</td>\n",
       "      <td>[7.06954, 8.22206, 10.788137, 10.460276, 9.97...</td>\n",
       "      <td>[3.56195, 6.2237577, 10.590099, 9.364218, 7.9...</td>\n",
       "      <td>[2.264758, 2.9242005, 8.381964, 7.1470985, 6....</td>\n",
       "      <td>[2.5808952, 2.183845, 5.509285, 4.1491947, 4....</td>\n",
       "      <td>[2.9141243, 3.7477784, 3.8442445, 2.7395709, ...</td>\n",
       "      <td>[3.6552742, 3.716537, 1.9041413, 2.350606, 2....</td>\n",
       "      <td>[2.542642, 0.7222822, -0.16030657, 0.7033057,...</td>\n",
       "      <td>[-454.5815, -481.88647, -506.8154, -505.86523...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.135778</td>\n",
       "      <td>0.319711</td>\n",
       "      <td>1894.964967</td>\n",
       "      <td>1746.052445</td>\n",
       "      <td>3657.568359</td>\n",
       "      <td>0.044638</td>\n",
       "      <td>[93.67096, 105.87641, 120.79372, 113.927605, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[9.562181, 9.880527, 5.5669446, 8.13823, 6.80...</td>\n",
       "      <td>[-12.6015415, -9.824036, -10.921469, -8.58217...</td>\n",
       "      <td>[-9.072727, -2.789328, -4.1653934, -4.451466,...</td>\n",
       "      <td>[15.360479, 17.082771, 17.751684, 20.176128, ...</td>\n",
       "      <td>[12.586064, 5.655569, 8.9193, 12.227798, 15.0...</td>\n",
       "      <td>[3.9960914, -0.9085444, -1.3349696, -3.413934...</td>\n",
       "      <td>[9.923491, 13.2172365, 12.597883, 3.8224878, ...</td>\n",
       "      <td>[9.994133, 11.891722, 12.108128, 4.8265734, 0...</td>\n",
       "      <td>[7.0713415, 5.8047085, 1.4041085, 1.0294272, ...</td>\n",
       "      <td>[-409.63712, -386.5259, -378.2063, -387.13794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.130460</td>\n",
       "      <td>0.343494</td>\n",
       "      <td>2256.247714</td>\n",
       "      <td>1754.663927</td>\n",
       "      <td>4012.988713</td>\n",
       "      <td>0.076340</td>\n",
       "      <td>[97.00813, 95.66066, 88.584274, 84.65976, 59....</td>\n",
       "      <td>...</td>\n",
       "      <td>[17.515745, 16.838634, 19.745289, 20.381605, ...</td>\n",
       "      <td>[1.8015573, 3.1914775, 11.76167, 13.773386, 9...</td>\n",
       "      <td>[-8.903702, -6.5261126, -1.8068645, -1.486261...</td>\n",
       "      <td>[5.7993836, 11.223903, 13.586725, 8.191362, 0...</td>\n",
       "      <td>[8.346506, 8.879691, 11.701046, 9.64171, 3.07...</td>\n",
       "      <td>[-1.5534915, -6.4333105, -5.056478, -5.968287...</td>\n",
       "      <td>[0.9691664, 0.7823979, 1.2080759, -5.55066, -...</td>\n",
       "      <td>[7.1364594, 7.3048954, 5.3913317, 0.72815526,...</td>\n",
       "      <td>[5.746996, 2.0462205, -0.24056518, 0.99955547...</td>\n",
       "      <td>[-460.30096, -446.96036, -452.4424, -474.6782...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   key  duration   rate      rmse  chroma_stft    spec_cent      spec_bw  \\\n",
       "0    0       3.9  44100  0.140494     0.315876  1866.409879  1589.100426   \n",
       "1    1       3.9  44100  0.162743     0.321692  1900.991630  1655.337451   \n",
       "2    2       3.9  44100  0.164615     0.355232  2063.848819  1775.474533   \n",
       "3    3       3.9  44100  0.168792     0.451515  1852.997999  1686.271162   \n",
       "4    4       3.9  44100  0.154860     0.319131  2039.119501  1727.509904   \n",
       "5    5       3.9  44100  0.135778     0.319711  1894.964967  1746.052445   \n",
       "6    6       3.9  44100  0.130460     0.343494  2256.247714  1754.663927   \n",
       "\n",
       "       rolloff       zcr                                             mfcc-1  \\\n",
       "0  3359.724832  0.054397   [83.27722, 99.38664, 102.47901, 90.34943, 71....   \n",
       "1  3481.883645  0.049550   [41.19271, 43.64393, 39.571014, 39.280922, 37...   \n",
       "2  3859.115658  0.049307   [82.287254, 87.646355, 82.0253, 94.94374, 91....   \n",
       "3  3497.119321  0.048648   [119.83493, 107.27291, 75.60173, 67.41678, 67...   \n",
       "4  3764.721680  0.053856   [89.52852, 70.175644, 45.881165, 45.599052, 4...   \n",
       "5  3657.568359  0.044638   [93.67096, 105.87641, 120.79372, 113.927605, ...   \n",
       "6  4012.988713  0.076340   [97.00813, 95.66066, 88.584274, 84.65976, 59....   \n",
       "\n",
       "   ...                                            mfcc-11  \\\n",
       "0  ...   [8.168702, 14.0366535, 16.038925, 15.536394, ...   \n",
       "1  ...   [12.310079, 11.866571, 14.6186, 13.844271, 15...   \n",
       "2  ...   [5.2754736, 3.1030126, 13.407595, 11.497463, ...   \n",
       "3  ...   [6.872613, 13.56502, 21.499748, 20.410894, 19...   \n",
       "4  ...   [4.9950647, 8.634994, 14.272382, 12.56731, 12...   \n",
       "5  ...   [9.562181, 9.880527, 5.5669446, 8.13823, 6.80...   \n",
       "6  ...   [17.515745, 16.838634, 19.745289, 20.381605, ...   \n",
       "\n",
       "                                             mfcc-12  \\\n",
       "0   [-3.862731, -4.396248, 5.0084095, 10.777882, ...   \n",
       "1   [9.388332, 9.306625, 11.078758, 10.565126, 13...   \n",
       "2   [-3.8446255, -2.971214, -4.5158086, -15.05637...   \n",
       "3   [1.4193982, 6.754388, 12.58844, 9.958288, 8.7...   \n",
       "4   [7.4182777, 7.690419, 12.692128, 12.182736, 1...   \n",
       "5   [-12.6015415, -9.824036, -10.921469, -8.58217...   \n",
       "6   [1.8015573, 3.1914775, 11.76167, 13.773386, 9...   \n",
       "\n",
       "                                             mfcc-13  \\\n",
       "0   [-5.1126146, -8.593059, 0.5960377, 3.3219213,...   \n",
       "1   [6.2646713, 6.783835, 7.9363847, 7.3178806, 1...   \n",
       "2   [-10.348606, -7.0784025, -4.1666775, -4.44252...   \n",
       "3   [8.313048, 11.108404, 11.795284, 11.0210495, ...   \n",
       "4   [7.06954, 8.22206, 10.788137, 10.460276, 9.97...   \n",
       "5   [-9.072727, -2.789328, -4.1653934, -4.451466,...   \n",
       "6   [-8.903702, -6.5261126, -1.8068645, -1.486261...   \n",
       "\n",
       "                                             mfcc-14  \\\n",
       "0   [7.466035, 8.751228, 11.057912, 8.758237, 9.4...   \n",
       "1   [3.6419904, 4.6776457, 6.5209513, 5.654462, 9...   \n",
       "2   [-6.6246963, -4.2140656, 7.8236904, 18.939777...   \n",
       "3   [6.1494207, 13.033355, 11.653352, 12.482355, ...   \n",
       "4   [3.56195, 6.2237577, 10.590099, 9.364218, 7.9...   \n",
       "5   [15.360479, 17.082771, 17.751684, 20.176128, ...   \n",
       "6   [5.7993836, 11.223903, 13.586725, 8.191362, 0...   \n",
       "\n",
       "                                             mfcc-15  \\\n",
       "0   [7.371366, 6.87022, 10.244551, 11.825554, 6.3...   \n",
       "1   [2.104116, 3.356882, 5.659413, 4.1505423, 7.4...   \n",
       "2   [3.9710789, 5.172476, 7.676926, 8.67643, 14.1...   \n",
       "3   [-0.7938506, 2.4656396, 4.9645514, 4.650465, ...   \n",
       "4   [2.264758, 2.9242005, 8.381964, 7.1470985, 6....   \n",
       "5   [12.586064, 5.655569, 8.9193, 12.227798, 15.0...   \n",
       "6   [8.346506, 8.879691, 11.701046, 9.64171, 3.07...   \n",
       "\n",
       "                                             mfcc-16  \\\n",
       "0   [-3.9841778, -6.7652984, 0.2561152, 8.012002,...   \n",
       "1   [1.8616735, 3.0369122, 4.5197396, 2.1954632, ...   \n",
       "2   [8.910217, 11.790334, 7.967798, 1.5910062, 2....   \n",
       "3   [2.9275475, 3.0828507, 6.777918, 4.701215, 4....   \n",
       "4   [2.5808952, 2.183845, 5.509285, 4.1491947, 4....   \n",
       "5   [3.9960914, -0.9085444, -1.3349696, -3.413934...   \n",
       "6   [-1.5534915, -6.4333105, -5.056478, -5.968287...   \n",
       "\n",
       "                                             mfcc-17  \\\n",
       "0   [-10.074699, -8.62624, -2.4564838, 3.956565, ...   \n",
       "1   [2.6579711, 3.5943644, 4.0682106, 1.5907955, ...   \n",
       "2   [1.8036795, 5.0296683, 7.0958915, 3.4036198, ...   \n",
       "3   [1.3701015, 6.540661, 12.440676, 9.548298, 9....   \n",
       "4   [2.9141243, 3.7477784, 3.8442445, 2.7395709, ...   \n",
       "5   [9.923491, 13.2172365, 12.597883, 3.8224878, ...   \n",
       "6   [0.9691664, 0.7823979, 1.2080759, -5.55066, -...   \n",
       "\n",
       "                                             mfcc-18  \\\n",
       "0   [-9.044133, -6.4383917, -0.39757836, 2.210609...   \n",
       "1   [3.8751554, 4.5339994, 4.1353364, 2.3035893, ...   \n",
       "2   [-5.9104943, -4.2941203, -1.3336377, -5.77136...   \n",
       "3   [-1.563393, 1.938827, 7.003166, 6.8333797, 7....   \n",
       "4   [3.6552742, 3.716537, 1.9041413, 2.350606, 2....   \n",
       "5   [9.994133, 11.891722, 12.108128, 4.8265734, 0...   \n",
       "6   [7.1364594, 7.3048954, 5.3913317, 0.72815526,...   \n",
       "\n",
       "                                             mfcc-19  \\\n",
       "0   [-5.1926327, -0.0011684299, 6.8749275, 8.6460...   \n",
       "1   [4.78513, 5.23909, 3.1639218, 2.1374843, 3.07...   \n",
       "2   [-4.1684318, -3.8071477, -3.5682287, -4.74488...   \n",
       "3   [4.3837843, 3.7773314, 3.6287487, 6.9111385, ...   \n",
       "4   [2.542642, 0.7222822, -0.16030657, 0.7033057,...   \n",
       "5   [7.0713415, 5.8047085, 1.4041085, 1.0294272, ...   \n",
       "6   [5.746996, 2.0462205, -0.24056518, 0.99955547...   \n",
       "\n",
       "                                              mfcc-0  \n",
       "0   [-499.7574, -478.80606, -484.01407, -501.7097...  \n",
       "1   [-513.5918, -511.832, -514.46265, -514.7265, ...  \n",
       "2   [-474.19022, -459.1682, -450.55014, -418.1049...  \n",
       "3   [-378.3314, -411.32068, -453.55417, -462.8506...  \n",
       "4   [-454.5815, -481.88647, -506.8154, -505.86523...  \n",
       "5   [-409.63712, -386.5259, -378.2063, -387.13794...  \n",
       "6   [-460.30096, -446.96036, -452.4424, -474.6782...  \n",
       "\n",
       "[7 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baf182",
   "metadata": {},
   "source": [
    "## LSTM Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db5ee6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 04:00:08,649:logger:Successfully initialized clean class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] (size =27)\n",
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 193)]                         0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 193, 1)                        0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 97, 32)                        14432            \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 97, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 97, 32)                        0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 49, 32)                        236544           \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 49, 32)                        128              \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 49, 32)                        0                \n",
      "                                                                                                              \n",
      " reshape (Reshape)                               (None, None, 1568)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 1024)                          6395904          \n",
      "                                                                                                              \n",
      " dropout (Dropout)                               (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_2 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_1 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_3 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_2 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_4 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dropout_3 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " bidirectional_5 (Bidirectional)                 (None, None, 1024)                          4724736          \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 1024)                          1049600          \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dropout_4 (Dropout)                             (None, None, 1024)                          0                \n",
      "                                                                                                              \n",
      " dense (Dense)                                   (None, None, 28)                            28700            \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 26,624,380\n",
      "Trainable params: 26,624,252\n",
      "Non-trainable params: 128\n",
      "______________________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' [83.27722, 99.38664, 102.47901, 90.34943, 71.54837, 53.873425, 59.003654, 89.88258, 99.3022, 99.94652, 158.26758, 208.78708, 229.12656, 241.53789, 244.97302, 241.9643, 235.3359, 218.62299, 213.32831, 211.32208, 197.4524, 186.91382, 182.09975, 166.18806, 141.60298, 126.632935, 100.75958, 79.14693, 78.0766, 77.61401, 64.09589, 82.23514, 134.8732, 167.01694, 184.85373, 189.25708, 191.55435, 193.78966, 196.96954, 207.26431, 197.36087, 173.59033, 175.45404, 196.573, 202.77698, 196.40088, 192.08478, 188.46436, 179.6095, 161.25032, 137.50461, 125.87592, 152.9017, 171.32639, 172.54224, 168.08238, 162.65605, 148.36945, 126.09946, 113.51777, 98.660126, 103.057365, 137.34132, 166.3909, 186.65335, 189.99835, 186.18044, 185.20087, 188.07272, 193.12933, 196.53796, 192.27649, 186.30527, 186.48424, 192.17026, 182.10129, 162.25037, 166.64824, 185.00336, 184.87964, 163.90982, 152.71375, 149.48259, 155.5998, 156.44287, 149.54218, 148.63057, 170.91635, 184.70651, 182.77739, 180.61417, 183.45795, 183.39056, 193.33258, 198.06558, 207.45792, 209.94757, 195.5111, 195.81927, 201.80919, 209.05646, 201.767, 191.73944, 185.82423, 183.19284, 182.76767, 178.97876, 182.54239, 178.73894, 158.98524, 149.2956, 144.68204, 138.8883, 134.31937, 144.72696, 156.85794, 152.71713, 125.73485, 87.104355, 71.99604, 83.27076, 76.270966, 69.20776, 71.56024, 74.92749, 73.53516, 63.713943, 45.432526, 41.747498, 49.31682, 94.915985, 115.84129, 116.61046, 153.16478, 165.19518, 156.13388, 152.03847, 147.84169, 144.56738, 143.53387, 150.51834, 154.73175, 152.13347, 161.89456, 172.78673, 170.4059, 159.62991, 157.24098, 159.634, 157.59312, 162.10588, 191.37921, 215.86133, 213.36246, 201.16682, 204.72931, 203.46347, 203.52429, 200.07361, 194.52179, 192.50592, 188.53683, 184.62392, 185.71664, 189.63528, 195.85861, 201.4054, 203.30206, 207.23328, 207.9559, 212.73175, 214.48032, 209.17114, 204.37515, 211.1858, 210.17429, 205.59604, 201.92264, 199.65265, 200.28473, 197.99072, 201.54941, 202.07385, 206.77426, 210.87595, 215.5181, 215.99167, 199.5311, 170.21667, 150.04327, 146.64072, 137.59625, 130.06381, 125.24068, 123.24095, 123.85707, 121.74219, 120.45668, 123.62953, 129.45175, 169.76578, 221.12119, 229.53435, 220.93073, 216.38298, 213.31532, 208.04779, 200.95639, 194.2967, 193.54529, 191.80347, 177.42009, 160.19318, 144.24898, 132.17719, 125.02568, 121.64518, 121.931015, 125.63988, 123.48729, 125.786804, 130.13649, 131.13293, 134.88997, 170.58994, 203.75505, 212.79669, 202.10918, 188.03851, 185.44171, 181.7605, 190.37592, 202.03685, 193.60376, 180.20117, 196.45096, 195.64029]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m learn\u001b[38;5;241m.\u001b[39mbuild_asr_model(\n\u001b[1;32m      9\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mfft_length \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39mchar_to_num\u001b[38;5;241m.\u001b[39mvocabulary_size(),\n\u001b[1;32m     11\u001b[0m     rnn_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msummary(line_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m110\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:134\u001b[0m, in \u001b[0;36mDeepLearn.model\u001b[0;34m(self, model_, serialize)\u001b[0m\n\u001b[1;32m    130\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.runName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeep-learner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m learn \u001b[38;5;241m=\u001b[39m DeepLearn(input_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_width, label_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_width, shift\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift,\n\u001b[1;32m    132\u001b[0m          train_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df, val_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_df, test_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_df,\n\u001b[1;32m    133\u001b[0m          label_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_columns)\n\u001b[0;32m--> 134\u001b[0m inputs_, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_labels\u001b[49m\n\u001b[1;32m    135\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_and_fit(model, learn)\n\u001b[1;32m    136\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully executed the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:105\u001b[0m, in \u001b[0;36mDeepLearn.get_input_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_res\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# No example batch was found, so get one from the `.train` dataset\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m))\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# And cache it for next time\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_res \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:89\u001b[0m, in \u001b[0;36mDeepLearn.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@property\u001b[39m        \u001b[38;5;66;03m# Work out the label column indices.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:74\u001b[0m, in \u001b[0;36mDeepLearn.make_dataset\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    this function is responsible\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    for making the dataset\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtimeseries_dataset_from_array(\n\u001b[1;32m     76\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     77\u001b[0m         targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,)\n\u001b[1;32m     83\u001b[0m     ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_window)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/pandas/core/generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' [83.27722, 99.38664, 102.47901, 90.34943, 71.54837, 53.873425, 59.003654, 89.88258, 99.3022, 99.94652, 158.26758, 208.78708, 229.12656, 241.53789, 244.97302, 241.9643, 235.3359, 218.62299, 213.32831, 211.32208, 197.4524, 186.91382, 182.09975, 166.18806, 141.60298, 126.632935, 100.75958, 79.14693, 78.0766, 77.61401, 64.09589, 82.23514, 134.8732, 167.01694, 184.85373, 189.25708, 191.55435, 193.78966, 196.96954, 207.26431, 197.36087, 173.59033, 175.45404, 196.573, 202.77698, 196.40088, 192.08478, 188.46436, 179.6095, 161.25032, 137.50461, 125.87592, 152.9017, 171.32639, 172.54224, 168.08238, 162.65605, 148.36945, 126.09946, 113.51777, 98.660126, 103.057365, 137.34132, 166.3909, 186.65335, 189.99835, 186.18044, 185.20087, 188.07272, 193.12933, 196.53796, 192.27649, 186.30527, 186.48424, 192.17026, 182.10129, 162.25037, 166.64824, 185.00336, 184.87964, 163.90982, 152.71375, 149.48259, 155.5998, 156.44287, 149.54218, 148.63057, 170.91635, 184.70651, 182.77739, 180.61417, 183.45795, 183.39056, 193.33258, 198.06558, 207.45792, 209.94757, 195.5111, 195.81927, 201.80919, 209.05646, 201.767, 191.73944, 185.82423, 183.19284, 182.76767, 178.97876, 182.54239, 178.73894, 158.98524, 149.2956, 144.68204, 138.8883, 134.31937, 144.72696, 156.85794, 152.71713, 125.73485, 87.104355, 71.99604, 83.27076, 76.270966, 69.20776, 71.56024, 74.92749, 73.53516, 63.713943, 45.432526, 41.747498, 49.31682, 94.915985, 115.84129, 116.61046, 153.16478, 165.19518, 156.13388, 152.03847, 147.84169, 144.56738, 143.53387, 150.51834, 154.73175, 152.13347, 161.89456, 172.78673, 170.4059, 159.62991, 157.24098, 159.634, 157.59312, 162.10588, 191.37921, 215.86133, 213.36246, 201.16682, 204.72931, 203.46347, 203.52429, 200.07361, 194.52179, 192.50592, 188.53683, 184.62392, 185.71664, 189.63528, 195.85861, 201.4054, 203.30206, 207.23328, 207.9559, 212.73175, 214.48032, 209.17114, 204.37515, 211.1858, 210.17429, 205.59604, 201.92264, 199.65265, 200.28473, 197.99072, 201.54941, 202.07385, 206.77426, 210.87595, 215.5181, 215.99167, 199.5311, 170.21667, 150.04327, 146.64072, 137.59625, 130.06381, 125.24068, 123.24095, 123.85707, 121.74219, 120.45668, 123.62953, 129.45175, 169.76578, 221.12119, 229.53435, 220.93073, 216.38298, 213.31532, 208.04779, 200.95639, 194.2967, 193.54529, 191.80347, 177.42009, 160.19318, 144.24898, 132.17719, 125.02568, 121.64518, 121.931015, 125.63988, 123.48729, 125.786804, 130.13649, 131.13293, 134.88997, 170.58994, 203.75505, 212.79669, 202.10918, 188.03851, 185.44171, 181.7605, 190.37592, 202.03685, 193.60376, 180.20117, 196.45096, 195.64029]'"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['input_array'])\n",
    "fft_length = 384\n",
    "cleaner = Clean()\n",
    "vocabs = cleaner.vocab(EN_ALPHABET)\n",
    "char_to_num,num_to_char = vocabs\n",
    "model = learn.build_asr_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=512,\n",
    ")\n",
    "model.summary(line_length=110)\n",
    "predictions = learn.model(\n",
    "    model_=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87fcda",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b50a3",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b59de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c5a9f",
   "metadata": {},
   "source": [
    "## CNN Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b902656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:36:13.306843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:36:13.323140: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-01 12:36:13.446906: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-HP-EliteBook-Folio-9470m): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36058e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 26.0230 - mean_absolute_error: 3.9458 - val_loss: 8.4775 - val_mean_absolute_error: 2.9116\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 25.9675 - mean_absolute_error: 3.9411 - val_loss: 8.4363 - val_mean_absolute_error: 2.9045\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 25.9120 - mean_absolute_error: 3.9364 - val_loss: 8.3951 - val_mean_absolute_error: 2.8974\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 25.8567 - mean_absolute_error: 3.9316 - val_loss: 8.3541 - val_mean_absolute_error: 2.8903\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 25.8015 - mean_absolute_error: 3.9269 - val_loss: 8.3132 - val_mean_absolute_error: 2.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/01 10:57:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: 'MapDataset' object has no attribute '_batch_size'\n",
      "2022-06-01 10:57:05,090:logger:Successfully executed the model\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['text'])\n",
    "predictions = learn.model(\n",
    "    model_=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2b324",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c1d7b",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df371d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a7326",
   "metadata": {},
   "source": [
    "## CTC Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e464b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:36:13.306843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:36:13.323140: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-01 12:36:13.446906: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-HP-EliteBook-Folio-9470m): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c65062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 26.0230 - mean_absolute_error: 3.9458 - val_loss: 8.4775 - val_mean_absolute_error: 2.9116\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 25.9675 - mean_absolute_error: 3.9411 - val_loss: 8.4363 - val_mean_absolute_error: 2.9045\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 25.9120 - mean_absolute_error: 3.9364 - val_loss: 8.3951 - val_mean_absolute_error: 2.8974\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 25.8567 - mean_absolute_error: 3.9316 - val_loss: 8.3541 - val_mean_absolute_error: 2.8903\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 25.8015 - mean_absolute_error: 3.9269 - val_loss: 8.3132 - val_mean_absolute_error: 2.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/01 10:57:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: 'MapDataset' object has no attribute '_batch_size'\n",
      "2022-06-01 10:57:05,090:logger:Successfully executed the model\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['text'])\n",
    "predictions = learn.model(\n",
    "    model_=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75603f9a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407a636",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4526a9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
