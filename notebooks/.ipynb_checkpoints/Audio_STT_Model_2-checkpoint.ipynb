{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a862643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.46.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.6.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.5.18.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: keras in /home/martin/Documents/swahili_nlp/env/lib/python3.10/site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq tensorflow_addons\n",
    "!pip install -qqq tensorflow-io\n",
    "!pip install tensorflow\n",
    "!pip install -U keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c89673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import layers\n",
    "import warnings\n",
    "from tensorflow_addons import layers as addon_layers\n",
    "# Setting logger level to avoid input shape warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# Defining hyperparameters\n",
    "\n",
    "DESIRED_SAMPLES = 8192\n",
    "LEARNING_RATE_GEN = 1e-5\n",
    "LEARNING_RATE_DISC = 1e-6\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "mse = keras.losses.MeanSquaredError()\n",
    "mae = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1fa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE='swahili'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020c272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files: 12148\n"
     ]
    }
   ],
   "source": [
    "wavs = tf.io.gfile.glob(f\"../data/{LANGUAGE}_train_wav/*.wav\")\n",
    "print(f\"Number of audio files: {len(wavs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a65194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    audio = tf.audio.decode_wav(tf.io.read_file(filename), 1, DESIRED_SAMPLES).audio\n",
    "    return audio, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fdf1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 08:30:01.157752: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-09 08:30:01.157821: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-HP-EliteBook-Folio-9470m): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# Create tf.data.Dataset objects and apply preprocessing\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((wavs,))\n",
    "train_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380e17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom keras layer for on-the-fly audio to spectrogram conversion\n",
    "class MelSpec(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_length=1024,\n",
    "        frame_step=256,\n",
    "        fft_length=None,\n",
    "        sampling_rate=22050,\n",
    "        num_mel_channels=80,\n",
    "        freq_min=125,\n",
    "        freq_max=7600,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.fft_length = fft_length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.num_mel_channels = num_mel_channels\n",
    "        self.freq_min = freq_min\n",
    "        self.freq_max = freq_max\n",
    "        # Defining mel filter. This filter will be multiplied with the STFT output\n",
    "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=self.num_mel_channels,\n",
    "            num_spectrogram_bins=self.frame_length // 2 + 1,\n",
    "            sample_rate=self.sampling_rate,\n",
    "            lower_edge_hertz=self.freq_min,\n",
    "            upper_edge_hertz=self.freq_max,\n",
    "        )\n",
    "\n",
    "    def call(self, audio, training=True):\n",
    "        # We will only perform the transformation during training.\n",
    "        if training:\n",
    "            # Taking the Short Time Fourier Transform. Ensure that the audio is padded.\n",
    "            # In the paper, the STFT output is padded using the 'REFLECT' strategy.\n",
    "            stft = tf.signal.stft(\n",
    "                tf.squeeze(audio, -1),\n",
    "                self.frame_length,\n",
    "                self.frame_step,\n",
    "                self.fft_length,\n",
    "                pad_end=True,\n",
    "            )\n",
    "\n",
    "            # Taking the magnitude of the STFT output\n",
    "            magnitude = tf.abs(stft)\n",
    "\n",
    "            # Multiplying the Mel-filterbank with the magnitude and scaling it using the db scale\n",
    "            mel = tf.matmul(tf.square(magnitude), self.mel_filterbank)\n",
    "            log_mel_spec = tfio.audio.dbscale(mel, top_db=80)\n",
    "            return log_mel_spec\n",
    "        else:\n",
    "            return audio\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MelSpec, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"frame_length\": self.frame_length,\n",
    "                \"frame_step\": self.frame_step,\n",
    "                \"fft_length\": self.fft_length,\n",
    "                \"sampling_rate\": self.sampling_rate,\n",
    "                \"num_mel_channels\": self.num_mel_channels,\n",
    "                \"freq_min\": self.freq_min,\n",
    "                \"freq_max\": self.freq_max,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6935ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack(input, filters):\n",
    "    \"\"\"Convolutional residual stack with weight normalization.\n",
    "\n",
    "    Args:\n",
    "        filter: int, determines filter size for the residual stack.\n",
    "\n",
    "    Returns:\n",
    "        Residual stack output.\n",
    "    \"\"\"\n",
    "    c1 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n",
    "    )(input)\n",
    "    lrelu1 = layers.LeakyReLU()(c1)\n",
    "    c2 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n",
    "    )(lrelu1)\n",
    "    add1 = layers.Add()([c2, input])\n",
    "\n",
    "    lrelu2 = layers.LeakyReLU()(add1)\n",
    "    c3 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(filters, 3, dilation_rate=3, padding=\"same\"), data_init=False\n",
    "    )(lrelu2)\n",
    "    lrelu3 = layers.LeakyReLU()(c3)\n",
    "    c4 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n",
    "    )(lrelu3)\n",
    "    add2 = layers.Add()([add1, c4])\n",
    "\n",
    "    lrelu4 = layers.LeakyReLU()(add2)\n",
    "    c5 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(filters, 3, dilation_rate=9, padding=\"same\"), data_init=False\n",
    "    )(lrelu4)\n",
    "    lrelu5 = layers.LeakyReLU()(c5)\n",
    "    c6 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(filters, 3, dilation_rate=1, padding=\"same\"), data_init=False\n",
    "    )(lrelu5)\n",
    "    add3 = layers.Add()([c6, add2])\n",
    "\n",
    "    return add3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edffa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dilated convolutional block consisting of the Residual stack\n",
    "def conv_block(input, conv_dim, upsampling_factor):\n",
    "    \"\"\"Dilated Convolutional Block with weight normalization.\n",
    "\n",
    "    Args:\n",
    "        conv_dim: int, determines filter size for the block.\n",
    "        upsampling_factor: int, scale for upsampling.\n",
    "\n",
    "    Returns:\n",
    "        Dilated convolution block.\n",
    "    \"\"\"\n",
    "    conv_t = addon_layers.WeightNormalization(\n",
    "        layers.Conv1DTranspose(conv_dim, 16, upsampling_factor, padding=\"same\"),\n",
    "        data_init=False,\n",
    "    )(input)\n",
    "    lrelu1 = layers.LeakyReLU()(conv_t)\n",
    "    res_stack = residual_stack(lrelu1, conv_dim)\n",
    "    lrelu2 = layers.LeakyReLU()(res_stack)\n",
    "    return lrelu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a180fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(input):\n",
    "    conv1 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(16, 15, 1, \"same\"), data_init=False\n",
    "    )(input)\n",
    "    lrelu1 = layers.LeakyReLU()(conv1)\n",
    "    conv2 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(64, 41, 4, \"same\", groups=4), data_init=False\n",
    "    )(lrelu1)\n",
    "    lrelu2 = layers.LeakyReLU()(conv2)\n",
    "    conv3 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(256, 41, 4, \"same\", groups=16), data_init=False\n",
    "    )(lrelu2)\n",
    "    lrelu3 = layers.LeakyReLU()(conv3)\n",
    "    conv4 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(1024, 41, 4, \"same\", groups=64), data_init=False\n",
    "    )(lrelu3)\n",
    "    lrelu4 = layers.LeakyReLU()(conv4)\n",
    "    conv5 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(1024, 41, 4, \"same\", groups=256), data_init=False\n",
    "    )(lrelu4)\n",
    "    lrelu5 = layers.LeakyReLU()(conv5)\n",
    "    conv6 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(1024, 5, 1, \"same\"), data_init=False\n",
    "    )(lrelu5)\n",
    "    lrelu6 = layers.LeakyReLU()(conv6)\n",
    "    conv7 = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(1, 3, 1, \"same\"), data_init=False\n",
    "    )(lrelu6)\n",
    "    return [lrelu1, lrelu2, lrelu3, lrelu4, lrelu5, lrelu6, conv7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58bae6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " mel_spec_1 (MelSpec)           (None, None, 80)     0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, None, 512)    287232      ['mel_spec_1[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, None, 512)    0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " weight_normalization (WeightNo  (None, None, 256)   2097921     ['leaky_re_lu_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, None, 256)    0           ['weight_normalization[0][0]']   \n",
      "                                                                                                  \n",
      " weight_normalization_1 (Weight  (None, None, 256)   197121      ['leaky_re_lu_2[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, None, 256)    0           ['weight_normalization_1[0][0]'] \n",
      "                                                                                                  \n",
      " weight_normalization_2 (Weight  (None, None, 256)   197121      ['leaky_re_lu_3[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 256)    0           ['weight_normalization_2[0][0]', \n",
      "                                                                  'leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, None, 256)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " weight_normalization_3 (Weight  (None, None, 256)   197121      ['leaky_re_lu_4[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, None, 256)    0           ['weight_normalization_3[0][0]'] \n",
      "                                                                                                  \n",
      " weight_normalization_4 (Weight  (None, None, 256)   197121      ['leaky_re_lu_5[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, None, 256)    0           ['add[0][0]',                    \n",
      "                                                                  'weight_normalization_4[0][0]'] \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, None, 256)    0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_5 (Weight  (None, None, 256)   197121      ['leaky_re_lu_6[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, None, 256)    0           ['weight_normalization_5[0][0]'] \n",
      "                                                                                                  \n",
      " weight_normalization_6 (Weight  (None, None, 256)   197121      ['leaky_re_lu_7[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, None, 256)    0           ['weight_normalization_6[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, None, 256)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_7 (Weight  (None, None, 128)   524673      ['leaky_re_lu_8[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, None, 128)    0           ['weight_normalization_7[0][0]'] \n",
      "                                                                                                  \n",
      " weight_normalization_8 (Weight  (None, None, 128)   49409       ['leaky_re_lu_9[0][0]']          \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, None, 128)    0           ['weight_normalization_8[0][0]'] \n",
      "                                                                                                  \n",
      " weight_normalization_9 (Weight  (None, None, 128)   49409       ['leaky_re_lu_10[0][0]']         \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, None, 128)    0           ['weight_normalization_9[0][0]', \n",
      "                                                                  'leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, None, 128)    0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_10 (Weigh  (None, None, 128)   49409       ['leaky_re_lu_11[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, None, 128)    0           ['weight_normalization_10[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_11 (Weigh  (None, None, 128)   49409       ['leaky_re_lu_12[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, None, 128)    0           ['add_3[0][0]',                  \n",
      "                                                                  'weight_normalization_11[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, None, 128)    0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_12 (Weigh  (None, None, 128)   49409       ['leaky_re_lu_13[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, None, 128)    0           ['weight_normalization_12[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_13 (Weigh  (None, None, 128)   49409       ['leaky_re_lu_14[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, None, 128)    0           ['weight_normalization_13[0][0]',\n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, None, 128)    0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_14 (Weigh  (None, None, 64)    131265      ['leaky_re_lu_15[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_14[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_15 (Weigh  (None, None, 64)    12417       ['leaky_re_lu_16[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_15[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_16 (Weigh  (None, None, 64)    12417       ['leaky_re_lu_17[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, None, 64)     0           ['weight_normalization_16[0][0]',\n",
      "                                                                  'leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, None, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_17 (Weigh  (None, None, 64)    12417       ['leaky_re_lu_18[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_17[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_18 (Weigh  (None, None, 64)    12417       ['leaky_re_lu_19[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, None, 64)     0           ['add_6[0][0]',                  \n",
      "                                                                  'weight_normalization_18[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, None, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_19 (Weigh  (None, None, 64)    12417       ['leaky_re_lu_20[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_19[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_20 (Weigh  (None, None, 64)    12417       ['leaky_re_lu_21[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, None, 64)     0           ['weight_normalization_20[0][0]',\n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_22 (LeakyReLU)     (None, None, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_21 (Weigh  (None, None, 32)    32865       ['leaky_re_lu_22[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_23 (LeakyReLU)     (None, None, 32)     0           ['weight_normalization_21[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_22 (Weigh  (None, None, 32)    3137        ['leaky_re_lu_23[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, None, 32)     0           ['weight_normalization_22[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_23 (Weigh  (None, None, 32)    3137        ['leaky_re_lu_24[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, None, 32)     0           ['weight_normalization_23[0][0]',\n",
      "                                                                  'leaky_re_lu_23[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, None, 32)     0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " weight_normalization_24 (Weigh  (None, None, 32)    3137        ['leaky_re_lu_25[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_26 (LeakyReLU)     (None, None, 32)     0           ['weight_normalization_24[0][0]']\n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " weight_normalization_25 (Weigh  (None, None, 32)    3137        ['leaky_re_lu_26[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, None, 32)     0           ['add_9[0][0]',                  \n",
      "                                                                  'weight_normalization_25[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_27 (LeakyReLU)     (None, None, 32)     0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " weight_normalization_26 (Weigh  (None, None, 32)    3137        ['leaky_re_lu_27[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, None, 32)     0           ['weight_normalization_26[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_27 (Weigh  (None, None, 32)    3137        ['leaky_re_lu_28[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, None, 32)     0           ['weight_normalization_27[0][0]',\n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, None, 32)     0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " weight_normalization_28 (Weigh  (None, None, 1)     452         ['leaky_re_lu_29[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,646,912\n",
      "Trainable params: 4,646,658\n",
      "Non-trainable params: 254\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_generator(input_shape):\n",
    "    inp = keras.Input(input_shape)\n",
    "    x = MelSpec()(inp)\n",
    "    x = layers.Conv1D(512, 7, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = conv_block(x, 256, 8)\n",
    "    x = conv_block(x, 128, 8)\n",
    "    x = conv_block(x, 64, 2)\n",
    "    x = conv_block(x, 32, 2)\n",
    "    x = addon_layers.WeightNormalization(\n",
    "        layers.Conv1D(1, 7, padding=\"same\", activation=\"tanh\")\n",
    "    )(x)\n",
    "    return keras.Model(inp, x)\n",
    "\n",
    "\n",
    "# We use a dynamic input shape for the generator since the model is fully convolutional\n",
    "generator = create_generator((None, 1))\n",
    "generator.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2b0a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " average_pooling1d (AveragePool  (None, None, 1)     0           ['input_3[0][0]']                \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " average_pooling1d_1 (AveragePo  (None, None, 1)     0           ['average_pooling1d[0][0]']      \n",
      " oling1D)                                                                                         \n",
      "                                                                                                  \n",
      " weight_normalization_29 (Weigh  (None, None, 16)    273         ['input_3[0][0]']                \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_36 (Weigh  (None, None, 16)    273         ['average_pooling1d[0][0]']      \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_43 (Weigh  (None, None, 16)    273         ['average_pooling1d_1[0][0]']    \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_30 (LeakyReLU)     (None, None, 16)     0           ['weight_normalization_29[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, None, 16)     0           ['weight_normalization_36[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_42 (LeakyReLU)     (None, None, 16)     0           ['weight_normalization_43[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_30 (Weigh  (None, None, 64)    10625       ['leaky_re_lu_30[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_37 (Weigh  (None, None, 64)    10625       ['leaky_re_lu_36[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_44 (Weigh  (None, None, 64)    10625       ['leaky_re_lu_42[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_31 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_30[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_37[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_43 (LeakyReLU)     (None, None, 64)     0           ['weight_normalization_44[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_31 (Weigh  (None, None, 256)   42497       ['leaky_re_lu_31[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_38 (Weigh  (None, None, 256)   42497       ['leaky_re_lu_37[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_45 (Weigh  (None, None, 256)   42497       ['leaky_re_lu_43[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, None, 256)    0           ['weight_normalization_31[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_38 (LeakyReLU)     (None, None, 256)    0           ['weight_normalization_38[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, None, 256)    0           ['weight_normalization_45[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_32 (Weigh  (None, None, 1024)  169985      ['leaky_re_lu_32[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_39 (Weigh  (None, None, 1024)  169985      ['leaky_re_lu_38[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_46 (Weigh  (None, None, 1024)  169985      ['leaky_re_lu_44[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_32[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_39 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_39[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_46[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_33 (Weigh  (None, None, 1024)  169985      ['leaky_re_lu_33[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_40 (Weigh  (None, None, 1024)  169985      ['leaky_re_lu_39[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_47 (Weigh  (None, None, 1024)  169985      ['leaky_re_lu_45[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_34 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_33[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_40[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " leaky_re_lu_46 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_47[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_34 (Weigh  (None, None, 1024)  5244929     ['leaky_re_lu_34[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_41 (Weigh  (None, None, 1024)  5244929     ['leaky_re_lu_40[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_48 (Weigh  (None, None, 1024)  5244929     ['leaky_re_lu_46[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " leaky_re_lu_35 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_34[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_41[0][0]']\n",
      "                                                                                                  \n",
      " leaky_re_lu_47 (LeakyReLU)     (None, None, 1024)   0           ['weight_normalization_48[0][0]']\n",
      "                                                                                                  \n",
      " weight_normalization_35 (Weigh  (None, None, 1)     3075        ['leaky_re_lu_35[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_42 (Weigh  (None, None, 1)     3075        ['leaky_re_lu_41[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " weight_normalization_49 (Weigh  (None, None, 1)     3075        ['leaky_re_lu_47[0][0]']         \n",
      " tNormalization)                                                                                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,924,107\n",
      "Trainable params: 16,924,086\n",
      "Non-trainable params: 21\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_discriminator(input_shape):\n",
    "    inp = keras.Input(input_shape)\n",
    "    out_map1 = discriminator_block(inp)\n",
    "    pool1 = layers.AveragePooling1D()(inp)\n",
    "    out_map2 = discriminator_block(pool1)\n",
    "    pool2 = layers.AveragePooling1D()(pool1)\n",
    "    out_map3 = discriminator_block(pool2)\n",
    "    return keras.Model(inp, [out_map1, out_map2, out_map3])\n",
    "\n",
    "\n",
    "# We use a dynamic input shape for the discriminator\n",
    "# This is done because the input shape for the generator is unknown\n",
    "discriminator = create_discriminator((None, 1))\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e035f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "\n",
    "\n",
    "def generator_loss(real_pred, fake_pred):\n",
    "    \"\"\"Loss function for the generator.\n",
    "\n",
    "    Args:\n",
    "        real_pred: Tensor, output of the ground truth wave passed through the discriminator.\n",
    "        fake_pred: Tensor, output of the generator prediction passed through the discriminator.\n",
    "\n",
    "    Returns:\n",
    "        Loss for the generator.\n",
    "    \"\"\"\n",
    "    gen_loss = []\n",
    "    for i in range(len(fake_pred)):\n",
    "        gen_loss.append(mse(tf.ones_like(fake_pred[i][-1]), fake_pred[i][-1]))\n",
    "\n",
    "    return tf.reduce_mean(gen_loss)\n",
    "\n",
    "\n",
    "def feature_matching_loss(real_pred, fake_pred):\n",
    "    \"\"\"Implements the feature matching loss.\n",
    "\n",
    "    Args:\n",
    "        real_pred: Tensor, output of the ground truth wave passed through the discriminator.\n",
    "        fake_pred: Tensor, output of the generator prediction passed through the discriminator.\n",
    "\n",
    "    Returns:\n",
    "        Feature Matching Loss.\n",
    "    \"\"\"\n",
    "    fm_loss = []\n",
    "    for i in range(len(fake_pred)):\n",
    "        for j in range(len(fake_pred[i]) - 1):\n",
    "            fm_loss.append(mae(real_pred[i][j], fake_pred[i][j]))\n",
    "\n",
    "    return tf.reduce_mean(fm_loss)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_pred, fake_pred):\n",
    "    \"\"\"Implements the discriminator loss.\n",
    "\n",
    "    Args:\n",
    "        real_pred: Tensor, output of the ground truth wave passed through the discriminator.\n",
    "        fake_pred: Tensor, output of the generator prediction passed through the discriminator.\n",
    "\n",
    "    Returns:\n",
    "        Discriminator Loss.\n",
    "    \"\"\"\n",
    "    real_loss, fake_loss = [], []\n",
    "    for i in range(len(real_pred)):\n",
    "        real_loss.append(mse(tf.ones_like(real_pred[i][-1]), real_pred[i][-1]))\n",
    "        fake_loss.append(mse(tf.zeros_like(fake_pred[i][-1]), fake_pred[i][-1]))\n",
    "\n",
    "    # Calculating the final discriminator loss after scaling\n",
    "    disc_loss = tf.reduce_mean(real_loss) + tf.reduce_mean(fake_loss)\n",
    "    return disc_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6e7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelGAN(keras.Model):\n",
    "    def __init__(self, generator, discriminator, **kwargs):\n",
    "        \"\"\"MelGAN trainer class\n",
    "\n",
    "        Args:\n",
    "            generator: keras.Model, Generator model\n",
    "            discriminator: keras.Model, Discriminator model\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_optimizer,\n",
    "        disc_optimizer,\n",
    "        generator_loss,\n",
    "        feature_matching_loss,\n",
    "        discriminator_loss,\n",
    "    ):\n",
    "        \"\"\"MelGAN compile method.\n",
    "\n",
    "        Args:\n",
    "            gen_optimizer: keras.optimizer, optimizer to be used for training\n",
    "            disc_optimizer: keras.optimizer, optimizer to be used for training\n",
    "            generator_loss: callable, loss function for generator\n",
    "            feature_matching_loss: callable, loss function for feature matching\n",
    "            discriminator_loss: callable, loss function for discriminator\n",
    "        \"\"\"\n",
    "        super().compile()\n",
    "\n",
    "        # Optimizers\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "\n",
    "        # Losses\n",
    "        self.generator_loss = generator_loss\n",
    "        self.feature_matching_loss = feature_matching_loss\n",
    "        self.discriminator_loss = discriminator_loss\n",
    "\n",
    "        # Trackers\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"gen_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"disc_loss\")\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        x_batch_train, y_batch_train = batch\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Generating the audio wave\n",
    "            gen_audio_wave = generator(x_batch_train, training=True)\n",
    "\n",
    "            # Generating the features using the discriminator\n",
    "            fake_pred = discriminator(y_batch_train)\n",
    "            real_pred = discriminator(gen_audio_wave)\n",
    "\n",
    "            # Calculating the generator losses\n",
    "            gen_loss = generator_loss(real_pred, fake_pred)\n",
    "            fm_loss = feature_matching_loss(real_pred, fake_pred)\n",
    "\n",
    "            # Calculating final generator loss\n",
    "            gen_fm_loss = gen_loss + 10 * fm_loss\n",
    "\n",
    "            # Calculating the discriminator losses\n",
    "            disc_loss = discriminator_loss(real_pred, fake_pred)\n",
    "\n",
    "        # Calculating and applying the gradients for generator and discriminator\n",
    "        grads_gen = gen_tape.gradient(gen_fm_loss, generator.trainable_weights)\n",
    "        grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_weights)\n",
    "        gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_weights))\n",
    "        disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_weights))\n",
    "\n",
    "        self.gen_loss_tracker.update_state(gen_fm_loss)\n",
    "        self.disc_loss_tracker.update_state(disc_loss)\n",
    "\n",
    "        return {\n",
    "            \"gen_loss\": self.gen_loss_tracker.result(),\n",
    "            \"disc_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = keras.optimizers.Adam(\n",
    "    LEARNING_RATE_GEN, beta_1=0.5, beta_2=0.9, clipnorm=1\n",
    ")\n",
    "disc_optimizer = keras.optimizers.Adam(\n",
    "    LEARNING_RATE_DISC, beta_1=0.5, beta_2=0.9, clipnorm=1\n",
    ")\n",
    "\n",
    "# Start training\n",
    "generator = create_generator((None, 1))\n",
    "discriminator = create_discriminator((None, 1))\n",
    "\n",
    "mel_gan = MelGAN(generator, discriminator)\n",
    "mel_gan.compile(\n",
    "    gen_optimizer,\n",
    "    disc_optimizer,\n",
    "    generator_loss,\n",
    "    feature_matching_loss,\n",
    "    discriminator_loss,\n",
    ")\n",
    "mel_gan.fit(\n",
    "    train_dataset.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE), epochs=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193eb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generator.predict(audio_sample, batch_size=32, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
