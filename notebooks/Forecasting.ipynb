{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7140172",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "Prediction of sales is the central task in this challenge. you want to predict daily sales in various stores up to 6 weeks ahead of time. This will help the company plan ahead of time. \n",
    "\n",
    "The following steps outline the various sub tasks needed to effectively do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing of libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import ticker\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import os,sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "from timeseries import TimeSeries\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664cbd1b",
   "metadata": {},
   "source": [
    "2.2 Building models with sklearn pipelines\n",
    "At this point, all our features are numeric. Since our problem is a regression problem, you can narrow down the list of algorithms you can use for modelling. \n",
    "\n",
    "A reasonable starting point will be to use any of the tree based algorithms. Random forests Regressor will make for a good start. \n",
    "\n",
    "Also, for the sake of this challenge, work with sklearn pipelines. This makes modeling modular and more reproducible. Working with pipelines will also significantly reduce your workload when you are moving your setup into files for the next part of the challenge. Extra marks will be awarded for doing this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/cleaned_train.csv\")\n",
    "test = pd.read_csv(\"../data/cleaned_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a1ff8",
   "metadata": {},
   "source": [
    "Unit Root Tests helps us determine the stationarity of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = TimeSeries(train)\n",
    "# timeseries.perform_adfuller('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8af4bf",
   "metadata": {},
   "source": [
    "Remove stationarity from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91486733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sales'] = timeseries.remove_stationarity(train.Sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66957c",
   "metadata": {},
   "source": [
    "Splitting of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30404278",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(train.Sales)\n",
    "WINDOW_SIZE = 489\n",
    "BATCH_SIZE= SIZE-WINDOW_SIZE*2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train.Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cd519",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetTrain = timeseries.split_dataset(train.Sales[0:BATCH_SIZE].values)\n",
    "DatasetVal = timeseries.split_dataset(train.Sales[BATCH_SIZE].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ebc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timeseries.model()\n",
    "results = model.fit(DatasetTrain, epochs=EPOCHS, validation_data=DatasetVal, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f303af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forecast = model_forecast(model, train.Sales.values[:, np.newaxis], WINDOW_SIZE)\n",
    "Results = Forecast[BATCH_SIZE-WINDOW_SIZE:-1]\n",
    "Results1 = scaler.inverse_transform(Results.reshape(-1,1))\n",
    "XValid1 = scaler.inverse_transform(XValid.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_forecast_= timeseries.view_forecast(self,DateValid,XValid1,Results1,Results,WINDOW_SIZE=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71eabca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
