{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a80245",
   "metadata": {},
   "source": [
    "# Modelling and Deployment using MLOps \n",
    "\n",
    "Now that we have audio input data & corresponding labels in an array format, it is easier to consume and apply Natural language processing techniques. We can convert audio files labels into integers using label Encoding or One Hot Vector Encoding for machines to learn. The labeled dataset will help us in the neural network model output layer for predicting results. These help in training & validation datasets into nD array.\n",
    "At this stage, we apply other pre-processing techniques like dropping columns, normalization, etc. to conclude our final training data for building models. Moving to the next stage of splitting the dataset into train, test, and validation is what we have been doing for other models. \n",
    "We can leverage CNN, RNN, LSTM,CTC etc. deep neural algorithms to build and train the models for speech applications like speech recognition. The model trained with the standard size few seconds audio chunk transformed into an array of n dimensions with the respective labels will result in predicting output labels for test audio input. As output labels will vary beyond binary, we are talking about building a multi-class label classification method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75aaa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "import tensorflow as tf\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81812",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880d08c",
   "metadata": {},
   "source": [
    "**objective**: Build a Deep learning model that converts speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f636c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_df = pd.read_csv(\"../data/swahili.csv\")\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")\n",
    "swahili_df['key'] = list(range(len(swahili_df)))\n",
    "amharic_df['key'] = list(range(len(amharic_df)))\n",
    "swahili_df['input_array'] = swahili_df['input_array'].apply(eval)\n",
    "amharic_df['input_array'] = amharic_df['input_array'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7188df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c907296",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_preprocessed = pre_model.preprocessing_learn(swahili_df,'text','input_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e2f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_preprocessed = pre_model.preprocessing_learn(amharic_df,'text','input_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d91ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df = swahili_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f886a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>duration</th>\n",
       "      <th>rate</th>\n",
       "      <th>rmse</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spec_cent</th>\n",
       "      <th>spec_bw</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zcr</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>input_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.140494</td>\n",
       "      <td>0.315876</td>\n",
       "      <td>1866.409879</td>\n",
       "      <td>1589.100426</td>\n",
       "      <td>3359.724832</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>-8.142890</td>\n",
       "      <td>[0.0002518580586183816, 2.563117777754087e-05,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.162743</td>\n",
       "      <td>0.321692</td>\n",
       "      <td>1900.991630</td>\n",
       "      <td>1655.337451</td>\n",
       "      <td>3481.883645</td>\n",
       "      <td>0.049550</td>\n",
       "      <td>-6.665521</td>\n",
       "      <td>[-0.0026284900959581137, -0.002864581765606999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.159490</td>\n",
       "      <td>0.456093</td>\n",
       "      <td>1420.839749</td>\n",
       "      <td>1589.164230</td>\n",
       "      <td>2979.181985</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>-1.883745</td>\n",
       "      <td>[0.020822471007704735, 0.023905068635940552, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.164615</td>\n",
       "      <td>0.355232</td>\n",
       "      <td>2063.848819</td>\n",
       "      <td>1775.474533</td>\n",
       "      <td>3859.115658</td>\n",
       "      <td>0.049307</td>\n",
       "      <td>-7.063959</td>\n",
       "      <td>[0.002116759540513158, 0.0019456746522337198, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.451515</td>\n",
       "      <td>1852.997999</td>\n",
       "      <td>1686.271162</td>\n",
       "      <td>3497.119321</td>\n",
       "      <td>0.048648</td>\n",
       "      <td>-1.036142</td>\n",
       "      <td>[0.029533347114920616, 0.03362444043159485, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.154860</td>\n",
       "      <td>0.319131</td>\n",
       "      <td>2039.119501</td>\n",
       "      <td>1727.509904</td>\n",
       "      <td>3764.721680</td>\n",
       "      <td>0.053856</td>\n",
       "      <td>-7.143945</td>\n",
       "      <td>[-0.010967539623379707, -0.012645195238292217,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.135778</td>\n",
       "      <td>0.319711</td>\n",
       "      <td>1894.964967</td>\n",
       "      <td>1746.052445</td>\n",
       "      <td>3657.568359</td>\n",
       "      <td>0.044638</td>\n",
       "      <td>-6.545257</td>\n",
       "      <td>[0.008869340643286705, 0.010183916427195072, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.130460</td>\n",
       "      <td>0.343494</td>\n",
       "      <td>2256.247714</td>\n",
       "      <td>1754.663927</td>\n",
       "      <td>4012.988713</td>\n",
       "      <td>0.076340</td>\n",
       "      <td>-7.412744</td>\n",
       "      <td>[0.005443760193884373, 0.005900760181248188, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.168273</td>\n",
       "      <td>0.410232</td>\n",
       "      <td>1853.555270</td>\n",
       "      <td>1664.910052</td>\n",
       "      <td>3527.881898</td>\n",
       "      <td>0.055760</td>\n",
       "      <td>-2.864870</td>\n",
       "      <td>[-0.020646054297685623, -0.02523760311305523, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.14</td>\n",
       "      <td>44100</td>\n",
       "      <td>0.162844</td>\n",
       "      <td>0.321559</td>\n",
       "      <td>1967.845186</td>\n",
       "      <td>1731.059926</td>\n",
       "      <td>3754.424650</td>\n",
       "      <td>0.054829</td>\n",
       "      <td>-6.871382</td>\n",
       "      <td>[0.005849024746567011, 0.006830638274550438, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key  duration   rate      rmse  chroma_stft    spec_cent      spec_bw  \\\n",
       "0    0      3.14  44100  0.140494     0.315876  1866.409879  1589.100426   \n",
       "1    1      3.14  44100  0.162743     0.321692  1900.991630  1655.337451   \n",
       "2    2      3.14  44100  0.159490     0.456093  1420.839749  1589.164230   \n",
       "3    3      3.14  44100  0.164615     0.355232  2063.848819  1775.474533   \n",
       "4    4      3.14  44100  0.168792     0.451515  1852.997999  1686.271162   \n",
       "5    5      3.14  44100  0.154860     0.319131  2039.119501  1727.509904   \n",
       "6    6      3.14  44100  0.135778     0.319711  1894.964967  1746.052445   \n",
       "7    7      3.14  44100  0.130460     0.343494  2256.247714  1754.663927   \n",
       "8    8      3.14  44100  0.168273     0.410232  1853.555270  1664.910052   \n",
       "9    9      3.14  44100  0.162844     0.321559  1967.845186  1731.059926   \n",
       "\n",
       "       rolloff       zcr      mfcc  \\\n",
       "0  3359.724832  0.054397 -8.142890   \n",
       "1  3481.883645  0.049550 -6.665521   \n",
       "2  2979.181985  0.032641 -1.883745   \n",
       "3  3859.115658  0.049307 -7.063959   \n",
       "4  3497.119321  0.048648 -1.036142   \n",
       "5  3764.721680  0.053856 -7.143945   \n",
       "6  3657.568359  0.044638 -6.545257   \n",
       "7  4012.988713  0.076340 -7.412744   \n",
       "8  3527.881898  0.055760 -2.864870   \n",
       "9  3754.424650  0.054829 -6.871382   \n",
       "\n",
       "                                         input_array  \n",
       "0  [0.0002518580586183816, 2.563117777754087e-05,...  \n",
       "1  [-0.0026284900959581137, -0.002864581765606999...  \n",
       "2  [0.020822471007704735, 0.023905068635940552, 0...  \n",
       "3  [0.002116759540513158, 0.0019456746522337198, ...  \n",
       "4  [0.029533347114920616, 0.03362444043159485, 0....  \n",
       "5  [-0.010967539623379707, -0.012645195238292217,...  \n",
       "6  [0.008869340643286705, 0.010183916427195072, 0...  \n",
       "7  [0.005443760193884373, 0.005900760181248188, 0...  \n",
       "8  [-0.020646054297685623, -0.02523760311305523, ...  \n",
       "9  [0.005849024746567011, 0.006830638274550438, 0...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baf182",
   "metadata": {},
   "source": [
    "## LSTM Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d062ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 13:43:19.513748: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-01 13:43:19.513891: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-01 13:43:19.514019: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-HP-EliteBook-Folio-9470m): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5ee6a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m learn \u001b[38;5;241m=\u001b[39m DeepLearn(input_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, label_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shift\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      2\u001b[0m                  train_df\u001b[38;5;241m=\u001b[39mtrain_df, val_df\u001b[38;5;241m=\u001b[39mval_df, test_df\u001b[38;5;241m=\u001b[39mtest_df,\n\u001b[1;32m      3\u001b[0m                  label_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_array\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:130\u001b[0m, in \u001b[0;36mDeepLearn.model\u001b[0;34m(self, model_, serialize)\u001b[0m\n\u001b[1;32m    126\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.runName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeep-learner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m learn \u001b[38;5;241m=\u001b[39m DeepLearn(input_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_width, label_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_width, shift\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift,\n\u001b[1;32m    128\u001b[0m          train_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df, val_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_df, test_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_df,\n\u001b[1;32m    129\u001b[0m          label_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_columns)\n\u001b[0;32m--> 130\u001b[0m inputs_, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_labels\u001b[49m\n\u001b[1;32m    131\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_and_fit(model, learn)\n\u001b[1;32m    132\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully executed the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:101\u001b[0m, in \u001b[0;36mDeepLearn.get_input_labels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_res\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# No example batch was found, so get one from the `.train` dataset\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m))\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# And cache it for next time\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_res \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:85\u001b[0m, in \u001b[0;36mDeepLearn.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@property\u001b[39m        \u001b[38;5;66;03m# Work out the label column indices.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/scripts/deep_learner.py:70\u001b[0m, in \u001b[0;36mDeepLearn.make_dataset\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    this function is responsible\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    for making the dataset\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtimeseries_dataset_from_array(\n\u001b[1;32m     72\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     73\u001b[0m         targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     77\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,)\n\u001b[1;32m     79\u001b[0m     ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_window)\n",
      "File \u001b[0;32m~/Documents/swahili_nlp/env/lib/python3.10/site-packages/pandas/core/generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['input_array'])\n",
    "predictions = learn.model(\n",
    "    model_=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87fcda",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b50a3",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b59de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c5a9f",
   "metadata": {},
   "source": [
    "## CNN Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b902656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:36:13.306843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:36:13.323140: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-01 12:36:13.446906: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-HP-EliteBook-Folio-9470m): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36058e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 26.0230 - mean_absolute_error: 3.9458 - val_loss: 8.4775 - val_mean_absolute_error: 2.9116\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 25.9675 - mean_absolute_error: 3.9411 - val_loss: 8.4363 - val_mean_absolute_error: 2.9045\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 25.9120 - mean_absolute_error: 3.9364 - val_loss: 8.3951 - val_mean_absolute_error: 2.8974\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 25.8567 - mean_absolute_error: 3.9316 - val_loss: 8.3541 - val_mean_absolute_error: 2.8903\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 25.8015 - mean_absolute_error: 3.9269 - val_loss: 8.3132 - val_mean_absolute_error: 2.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/01 10:57:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: 'MapDataset' object has no attribute '_batch_size'\n",
      "2022-06-01 10:57:05,090:logger:Successfully executed the model\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['text'])\n",
    "predictions = learn.model(\n",
    "    model_=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2b324",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c1d7b",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df371d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a7326",
   "metadata": {},
   "source": [
    "## CTC Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e464b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 12:36:13.306843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-01 12:36:13.323140: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-01 12:36:13.446906: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (martin-HP-EliteBook-Folio-9470m): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c65062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 26.0230 - mean_absolute_error: 3.9458 - val_loss: 8.4775 - val_mean_absolute_error: 2.9116\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 25.9675 - mean_absolute_error: 3.9411 - val_loss: 8.4363 - val_mean_absolute_error: 2.9045\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 25.9120 - mean_absolute_error: 3.9364 - val_loss: 8.3951 - val_mean_absolute_error: 2.8974\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 25.8567 - mean_absolute_error: 3.9316 - val_loss: 8.3541 - val_mean_absolute_error: 2.8903\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 25.8015 - mean_absolute_error: 3.9269 - val_loss: 8.3132 - val_mean_absolute_error: 2.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/06/01 10:57:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: 'MapDataset' object has no attribute '_batch_size'\n",
      "2022-06-01 10:57:05,090:logger:Successfully executed the model\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['text'])\n",
    "predictions = learn.model(\n",
    "    model_=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75603f9a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407a636",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4526a9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]],\n",
       " [[0.11674367636442184]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
