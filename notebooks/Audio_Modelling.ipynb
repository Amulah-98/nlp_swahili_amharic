{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a80245",
   "metadata": {},
   "source": [
    "# Modelling and Deployment using MLOps \n",
    "\n",
    "Now that we have audio input data & corresponding labels in an array format, it is easier to consume and apply Natural language processing techniques. We can convert audio files labels into integers using label Encoding or One Hot Vector Encoding for machines to learn. The labeled dataset will help us in the neural network model output layer for predicting results. These help in training & validation datasets into nD array.\n",
    "At this stage, we apply other pre-processing techniques like dropping columns, normalization, etc. to conclude our final training data for building models. Moving to the next stage of splitting the dataset into train, test, and validation is what we have been doing for other models. \n",
    "We can leverage CNN, RNN, LSTM,CTC etc. deep neural algorithms to build and train the models for speech applications like speech recognition. The model trained with the standard size few seconds audio chunk transformed into an array of n dimensions with the respective labels will result in predicting output labels for test audio input. As output labels will vary beyond binary, we are talking about building a multi-class label classification method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75aaa220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟ', 'ዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧ', 'የ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔ', 'ጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯ', 'ጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷ', 'ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ'] (size =220)\n",
      "c:\\Users\\Sebli\\Desktop\\10x Files\\Week 4\\nlp_swahili_amharic\\notebooks\n",
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "import tensorflow as tf\n",
    "from clean import Clean\n",
    "from utils import vocab\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler\n",
    "from evaluator import CallbackEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39c85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94abcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_ALPHABET = \"\"\"\n",
    "ሀ ሁ ሂ ሄ ህ ሆ\n",
    "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "ኋ\n",
    "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "አ ኡ ኢ ኤ እ ኦ\n",
    "ኧ\n",
    "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
    "ኳ\n",
    "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "ጸ ጹ ጺ ጻ ጼ ጽ ጾ\n",
    "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78298269",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_ALPHABET='abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d2ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 02:48:02,473:logger:Successfully initialized clean class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟ', 'ዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧ', 'የ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔ', 'ጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯ', 'ጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷ', 'ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ'] (size =220)\n"
     ]
    }
   ],
   "source": [
    "cleaner = Clean()\n",
    "char_to_num,num_to_char=vocab(AM_ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1bd62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function utils.vocab(alphabet)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81812",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880d08c",
   "metadata": {},
   "source": [
    "**objective**: Build a Deep learning model that converts speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f636c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_df = pd.read_csv('../data/swahili.csv')\n",
    "lang = pd.read_csv(\"../data/swahili.csv\")\n",
    "lang['type']='swahili'\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")\n",
    "amharic_df['type']='amharic'\n",
    "language_df = lang.append(amharic_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe74d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f37929",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_preprocessed = pre_model.preprocessing_learn(swahili_df,'key','file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83b3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_preprocessed = pre_model.preprocessing_learn(amharic_df,'key','file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00fc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df = amharic_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6ecf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(train_df[\"file\"]), list(train_df[\"text\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(val_df[\"file\"]), list(val_df[\"text\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(cleaner.encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0baf182",
   "metadata": {},
   "source": [
    "## Deep Learnin Architecture - CNN - RNN - LSTM & CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5ee6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DeepSpeech_2\"\n",
      "______________________________________________________________________________________________________________\n",
      " Layer (type)                                    Output Shape                                Param #          \n",
      "==============================================================================================================\n",
      " input (InputLayer)                              [(None, None, 2)]                           0                \n",
      "                                                                                                              \n",
      " expand_dim (Reshape)                            (None, None, 2, 1)                          0                \n",
      "                                                                                                              \n",
      " conv_1 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_1_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_1_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " conv_2 (Conv2D)                                 (None, None, 1, 2)                          4                \n",
      "                                                                                                              \n",
      " conv_2_bn (BatchNormalization)                  (None, None, 1, 2)                          8                \n",
      "                                                                                                              \n",
      " conv_2_relu (ReLU)                              (None, None, 1, 2)                          0                \n",
      "                                                                                                              \n",
      " reshape (Reshape)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " bidirectional_1 (Bidirectional)                 (None, None, 2)                             30               \n",
      "                                                                                                              \n",
      " dense_1 (Dense)                                 (None, None, 2)                             6                \n",
      "                                                                                                              \n",
      " dense_1_relu (ReLU)                             (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dropout (Dropout)                               (None, None, 2)                             0                \n",
      "                                                                                                              \n",
      " dense (Dense)                                   (None, None, 221)                           663              \n",
      "                                                                                                              \n",
      "==============================================================================================================\n",
      "Total params: 723\n",
      "Trainable params: 715\n",
      "Non-trainable params: 8\n",
      "______________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learn = DeepLearn(input_width=1, label_width=1, shift=1,epochs=5,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=['mfcc-0'])\n",
    "fft_length = 2\n",
    "model = learn.build_asr_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=1,\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87fcda",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b50a3",
   "metadata": {},
   "source": [
    "**objective**: Evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b59de5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - ETA: 0s - loss: 165201.9688  The vocabulary is: ['', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟ', 'ዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧ', 'የ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔ', 'ጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯ', 'ጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷ', 'ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ'] (size =220)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "The vocabulary is: ['', 'ሀ', 'ሁ', 'ሂ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሹ', 'ሺ', 'ሻ', 'ሼ', 'ሽ', 'ሾ', 'ሿ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቄ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቩ', 'ቪ', 'ቫ', 'ቬ', 'ቭ', 'ቮ', 'ቯ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኚ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'ኟ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ወ', 'ዉ', 'ዊ', 'ዋ', 'ዌ', 'ው', 'ዎ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዟ', 'ዠ', 'ዡ', 'ዢ', 'ዣ', 'ዤ', 'ዥ', 'ዦ', 'ዧ', 'የ', 'ዩ', 'ዪ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጐ', 'ጓ', 'ጔ', 'ጠ', 'ጡ', 'ጢ', 'ጣ', 'ጤ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጪ', 'ጫ', 'ጬ', 'ጭ', 'ጮ', 'ጯ', 'ጰ', 'ጱ', 'ጲ', 'ጳ', 'ጴ', 'ጵ', 'ጶ', 'ጷ', 'ጸ', 'ጹ', 'ጺ', 'ጻ', 'ጼ', 'ጽ', 'ጾ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፐ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ'] (size =220)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Word Error Rate: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target    : በሩንእንዲህበሀይልአታንኳብዬአልነበረምእንዴ\n",
      "Prediction: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Target    : በለጠችየበየነየበኩርልጅነች\n",
      "Prediction: \n",
      "----------------------------------------------------------------------------------------------------\n",
      "3/3 [==============================] - 423s 111s/step - loss: 165201.9688 - val_loss: 118441.6406\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(model,validation_dataset)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bee2bc",
   "metadata": {},
   "source": [
    "## Model Space Exploration\n",
    "Using hyperparameter optimization by slightly modifying the architecture e.g. increasing and decreasing the number of layers to find the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5a981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CONV_RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 128)]       0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, None, 128, 1)      0         \n",
      "                                                                 \n",
      " cnn_0 (Conv2D)              (None, None, 122, 16)     800       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, None, 122, 16)     0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, None, 60, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bn_cnn_0 (BatchNormalizatio  (None, None, 60, 16)     64        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " cnn_1 (Conv2D)              (None, None, 56, 32)      12832     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, None, 56, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, None, 27, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " bn_cnn_1 (BatchNormalizatio  (None, None, 27, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " cnn_2 (Conv2D)              (None, None, 25, 64)      18496     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, None, 25, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, None, 12, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " bn_cnn_2 (BatchNormalizatio  (None, None, 12, 64)     256       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, None, 768)         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 1024)       3938304   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_0 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 1024)       4724736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_1 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 1024)       4724736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_2 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, None, 1024)       4724736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bn_rnn_3 (BatchNormalizatio  (None, None, 1024)       4096      \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 224)        229600    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 224)         0         \n",
      "                                                                 \n",
      " dense_1_relu (LeakyReLU)    (None, None, 224)         0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, None, 224)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,391,072\n",
      "Trainable params: 18,382,656\n",
      "Non-trainable params: 8,416\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filters = [16, 32, 64]\n",
    "kernels = [7, 5, 3] \n",
    "pool_sizes = [3, 3, 3]  \n",
    "mx_stride = [1, 1, 2]\n",
    "cnn_stride = 1 \n",
    "input_dim = 128\n",
    "\n",
    "CONV_RNN_Model = learn.cnn_rnn_model(input_dim, filters, kernels, pool_sizes, mx_stride, cnn_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dafds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734777c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(CONV_RNN_Model,validation_dataset)\n",
    "# Train the model\n",
    "history = CONV_RNN_Model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f08ea943be090a14eff5269cda570ac55dcc0e2bd93317a14b7d7e20047a087"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
