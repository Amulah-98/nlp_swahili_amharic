{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4094a415",
   "metadata": {},
   "source": [
    "# Modelling and Deployment using MLOps \n",
    "\n",
    "Now that we have audio input data & corresponding labels in an array format, it is easier to consume and apply Natural language processing techniques. We can convert audio files labels into integers using label Encoding or One Hot Vector Encoding for machines to learn. The labeled dataset will help us in the neural network model output layer for predicting results. These help in training & validation datasets into nD array.\n",
    "At this stage, we apply other pre-processing techniques like dropping columns, normalization, etc. to conclude our final training data for building models. Moving to the next stage of splitting the dataset into train, test, and validation is what we have been doing for other models. The below diagram is a generic representation of the Convolution Neural Network.\n",
    "We can leverage CNN, RNN, LSTM, etc. deep neural algorithms to build and train the models for speech applications like speech recognition, emotion recognition, music genre classification, voice biometric, and many more. The model trained with the standard size few seconds audio chunk transformed into an array of n dimensions with the respective labels will result in predicting output labels for test audio input. As output labels will vary beyond binary, we are talking about building a multi-class label classification method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a03096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "from deep_learner import DeepLearn\n",
    "from modeling import Modeler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cd34e",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c46782",
   "metadata": {},
   "source": [
    "**objective**: Build a Deep learning model that converts speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a45bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_df = pd.read_csv(\"../data/swahili.csv\")\n",
    "amharic_df = pd.read_csv(\"../data/amharic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e033fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = Modeler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92301cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_preprocessed = pre_model.preprocessor_audio(swahili_df,'key','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ae7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "amharic_preprocessed = pre_model.preprocessor_audio(amharic_df,'key','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c172e3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  1.35012129,  2.8589185 , -0.45403679,\n",
       "         -0.19338387, -0.46378703, -0.26720165,  2.91466043],\n",
       "        [ 0.        ,  0.        , -0.6333291 , -0.48754577, -1.59989106,\n",
       "         -0.03052994, -0.95728472, -1.26394851, -0.49499576],\n",
       "        [ 0.        ,  0.        ,  0.88145886, -0.45618295,  0.26780785,\n",
       "          0.57184897,  0.65307092,  0.3661349 , -0.14130002],\n",
       "        [ 0.        ,  0.        ,  0.05497682, -0.34597687, -1.16898739,\n",
       "         -1.4174735 , -1.45522741, -0.94115513, -0.51615832],\n",
       "        [ 0.        ,  0.        ,  1.02100115,  0.40278805,  0.87121582,\n",
       "          1.33068917,  1.10749208, -0.19965215, -0.24215389],\n",
       "        [ 0.        ,  0.        , -1.66993095,  0.1033607 ,  2.08049324,\n",
       "          0.97513217,  1.77539254,  2.57029898, -0.42481577],\n",
       "        [ 0.        ,  0.        , -1.25093302, -0.5033259 , -0.1902634 ,\n",
       "          0.82800176,  0.2326571 , -0.67805882,  0.02949438],\n",
       "        [ 0.        ,  0.        , -0.87937071, -0.60113924, -0.36973961,\n",
       "         -1.85358233, -1.06016072,  0.3218741 , -0.80719865]]),\n",
       " array([[ 0.        ,  0.        ,  0.25244591, -0.51811499,  0.71578558,\n",
       "          0.5111955 ,  0.69776615,  0.26646686, -0.28404339],\n",
       "        [ 0.        ,  0.        ,  0.87355975, -0.45278152, -0.15238425,\n",
       "         -0.72189792, -0.52991891, -0.17475859, -0.03348902]]),\n",
       " array([1, 3, 8, 6, 0, 7, 2, 4]),\n",
       " array([5, 9]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swahili_preprocessed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
